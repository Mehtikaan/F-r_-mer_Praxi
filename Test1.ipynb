{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb81490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in /opt/anaconda3/lib/python3.12/site-packages (5.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install chardet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c19ad7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/umerhanif/Desktop/Praxisprojekt/F-r_-mer_Praxi/Test1.ipynb Zelle 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/umerhanif/Desktop/Praxisprojekt/F-r_-mer_Praxi/Test1.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mollama\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/umerhanif/Desktop/Praxisprojekt/F-r_-mer_Praxi/Test1.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mPyPDF2\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/umerhanif/Desktop/Praxisprojekt/F-r_-mer_Praxi/Test1.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpandas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ollama'"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\"\"\"\"\"\n",
    "# Excel einlesen\n",
    "df = pd.read_excel(\"dokument.xlsx\")\n",
    "\n",
    "# DataFrame zu Text konvertieren\n",
    "content = df.to_string()\n",
    "\n",
    "prompt=  f\"Return keywords for a database search to find patents like {content}, no explanation, nothing else just give us some keywordsback  .\"\n",
    "\n",
    "response=ollama.generate(model=\"qwen3:8b\", prompt=prompt)\n",
    "list_response_key=response[\"response\"]\n",
    "print(list_response_key)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import ollama\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "# 1. Excel einlesen\n",
    "df = pd.read_excel(\"dokument.xlsx\")\n",
    "\n",
    "# 2. Datenvalidierung: Pflichtfelder prüfen und unvollständige Zeilen entfernen\n",
    "pflichtfelder = [\"Abstract\", \"Title\", \"ApplicationDate\"]  # anpassen je nach Struktur\n",
    "\n",
    "zu_entfernende_indexe = []\n",
    "\n",
    "for idx, zeile in df.iterrows():\n",
    "    for feld in pflichtfelder:\n",
    "        if feld not in df.columns or pd.isna(zeile.get(feld)) or str(zeile.get(feld)).strip() == \"\" or len(str(zeile.get(feld))) < 20:\n",
    "            zu_entfernende_indexe.append(idx)\n",
    "            break  # sobald ein Pflichtfeld fehlt, reicht das\n",
    "\n",
    "df = df.drop(index=zu_entfernende_indexe)\n",
    "\n",
    "print(f\"{len(zu_entfernende_indexe)} ungültige Einträge entfernt.\")\n",
    "print(f\"{len(df)} gültige Einträge nach Validierung.\\n\")\n",
    "\n",
    "# 3. DataFrame zu String umwandeln\n",
    "content = df.to_string(index=False)\n",
    "\n",
    "# 4. Prompt an das Sprachmodell generieren\n",
    "prompt = (\n",
    "    f\"Return keywords for a database search to find patents like the following content:\\n\\n\"\n",
    "    f\"{content}\\n\\n\"\n",
    "    \"Return only the keywords, no explanation, no formatting, just plain text keywords.\"\n",
    ")\n",
    "\n",
    "# 5. Anfrage an Ollama-Modell senden (z. B. qwen3:8b)\n",
    "response = ollama.generate(model=\"qwen3:8b\", prompt=prompt)\n",
    "\n",
    "# 6. Ergebnis auslesen\n",
    "list_response_key = response[\"response\"]\n",
    "print(\"Extrahierte Keywords:\")\n",
    "print(list_response_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93da7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sende Anfrage an Ollama...\n",
      "Ollama Response:\n",
      "<think>\n",
      "Okay, let's tackle this. The user provided a long text about an electrochemical oxygen sensor and then a list of 20 claims. They want keywords related to the content. First, I need to understand the main points of the text.\n",
      "\n",
      "The text describes an electrochemical oxygen sensor with components like a positive electrode, negative electrode, electrolyte solution, separation membrane, and a resistance element. It mentions specific parameters like current values, resistance, and conditions for the electrolyte solution, such as using a chelating agent like citric acid. There are also claims that specify details like the separation membrane's thickness, materials, and ratios involving the electrolyte volume and tin content.\n",
      "\n",
      "Now, to extract keywords. The main components are the electrodes, electrolyte solution, separation membrane, and resistance element. The electrolyte solution's properties include chelating agents, citric acid, and specific concentrations. There are also technical parameters like current thresholds (7-10 microA), resistance values (1050-1500 ohms), humidity and temperature conditions (50% RH, 25°C), and the separation membrane's material (fluorocarbon resin) and thickness (8 micrometers). The claims mention ratios like x/y ≥ 0.3 (ml/g) involving electrolyte volume and tin content.\n",
      "\n",
      "I should list these components, materials, parameters, and specific terms. Make sure to include all the technical terms and specifications mentioned. Avoid any extra explanations, just the keywords. Let me check if I missed anything. The user wants keywords, so terms like \"electrochemical oxygen sensor,\" \"positive electrode,\" \"negative electrode,\" \"electrolyte solution,\" \"separation membrane,\" \"resistance element,\" \"chelating agent,\" \"citric acid,\" \"fluorocarbon resin,\" \"current value,\" \"resistance value,\" \"relative humidity,\" \"temperature,\" \"tin content,\" \"volume-to-weight ratio,\" etc., should be included. Also, the specific numbers like 7 microA, 1050 ohms, 50% RH, 25°C, 8 micrometers. Need to ensure all these are covered without any markdown.\n",
      "</think>\n",
      "\n",
      "electrochemical oxygen sensor, positive electrode, negative electrode, electrolyte solution, separation membrane, resistance element, chelating agent, citric acid, fluorocarbon resin, current value, resistance value, relative humidity, temperature, tin content, volume-to-weight ratio, fixed resistor, thermistor element, 50% RH, 25°C, 1 atm, 8 micrometers, 7 microA, 1050 ohms, 1500 ohms, 4-9.5 mV, x/y ≥ 0.3 (ml/g)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Gefundene zitierte Begriffe: 15\n",
      "Extrahierte Keywords mit OR:\n",
      "electrochemical oxygen sensor OR positive electrode OR negative electrode OR electrolyte solution OR separation membrane OR resistance element OR chelating agent OR citric acid OR fluorocarbon resin OR current value OR resistance value OR relative humidity OR temperature OR tin content OR volume-to-weight ratio OR humidity\n",
      "\n",
      "Anzahl Keywords: 16\n",
      "\n",
      "Ergebnis gespeichert in 'extracted_keywords.txt'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ollama\n",
    "import pandas as pd\n",
    "\n",
    "def extract_keywords_from_ollama_response(text):\n",
    "    \"\"\"\n",
    "    Extrahiert Keywords aus einer Ollama-Response und fügt OR zwischen jedem Wort ein.\n",
    "    Speziell für Text mit zitierten technischen Begriffen.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Die Ollama-Response als String\n",
    "    \n",
    "    Returns:\n",
    "        str: Keywords mit OR verknüpft\n",
    "    \"\"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    keywords = []\n",
    "    \n",
    "    # Muster 1: Begriffe in Anführungszeichen extrahieren\n",
    "    quoted_terms = re.findall(r'\"([^\"]+)\"', text)\n",
    "    if quoted_terms:\n",
    "        keywords.extend(quoted_terms)\n",
    "        print(f\"Gefundene zitierte Begriffe: {len(quoted_terms)}\")\n",
    "    \n",
    "    # Muster 2: Technische Begriffe nach \"like\" oder \"terms like\"\n",
    "    after_like = re.findall(r'(?:terms?\\s+like|like)\\s+\"([^\"]+)\"', text, re.IGNORECASE)\n",
    "    keywords.extend(after_like)\n",
    "    \n",
    "    # Muster 3: Begriffe nach \"include\" \n",
    "    after_include = re.findall(r'include[s]?\\s+\"([^\"]+)\"', text, re.IGNORECASE)\n",
    "    keywords.extend(after_include)\n",
    "    \n",
    "    # Muster 4: Zusätzliche technische Begriffe ohne Anführungszeichen\n",
    "    # Suche nach bekannten technischen Patterns\n",
    "    technical_patterns = [\n",
    "        r'\\bSn\\s+alloy\\b',\n",
    "        r'\\brelative\\s+humidity\\b', \n",
    "        r'\\bratio\\s+x/y\\b',\n",
    "        r'\\btemperature\\b',\n",
    "        r'\\bhumidity\\b',\n",
    "        r'\\bnumerical\\s+values\\b'\n",
    "    ]\n",
    "    \n",
    "    for pattern in technical_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        keywords.extend(matches)\n",
    "    \n",
    "    # Bereinige alle Keywords\n",
    "    cleaned_keywords = []\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        if isinstance(keyword, str):\n",
    "            # Grundbereinigung\n",
    "            clean_keyword = keyword.strip()\n",
    "            \n",
    "            # Entferne führende Artikel\n",
    "            clean_keyword = re.sub(r'^(the|a|an)\\s+', '', clean_keyword, flags=re.IGNORECASE)\n",
    "            \n",
    "            # Teile zusammengesetzte Begriffe bei Kommas\n",
    "            sub_keywords = [k.strip() for k in clean_keyword.split(',')]\n",
    "            \n",
    "            for sub_keyword in sub_keywords:\n",
    "                sub_keyword = sub_keyword.strip()\n",
    "                \n",
    "                # Filtere sehr kurze oder unerwünschte Begriffe\n",
    "                unwanted = ['and', 'or', 'the', 'specific', 'other', 'any', 'all']\n",
    "                \n",
    "                if (sub_keyword and \n",
    "                    len(sub_keyword) > 2 and \n",
    "                    sub_keyword.lower() not in unwanted):\n",
    "                    cleaned_keywords.append(sub_keyword)\n",
    "    \n",
    "    # Entferne Duplikate, behalte Reihenfolge\n",
    "    unique_keywords = list(dict.fromkeys(cleaned_keywords))\n",
    "    \n",
    "    # Füge OR zwischen allen Keywords ein\n",
    "    result = \" OR \".join(unique_keywords)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Excel einlesen\n",
    "        df = pd.read_excel(\"dokument.xlsx\")\n",
    "        \n",
    "        # DataFrame zu Text konvertieren\n",
    "        content = df.to_string()\n",
    "        \n",
    "        # Prompt für Ollama\n",
    "        prompt = f\"Return keywords for a database search to find patents like {content}. No explanation, nothing else, just give us some keywords back.\"\n",
    "        \n",
    "        print(\"Sende Anfrage an Ollama...\")\n",
    "        \n",
    "        # Ollama Response\n",
    "        response=ollama.generate(model=\"qwen3:8b\", prompt=prompt)\n",
    "        list_response = response[\"response\"]\n",
    "        \n",
    "        print(\"Ollama Response:\")\n",
    "        print(list_response)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Keywords extrahieren\n",
    "        keywords_with_or = extract_keywords_from_ollama_response(list_response)\n",
    "        \n",
    "        print(\"Extrahierte Keywords mit OR:\")\n",
    "        print(keywords_with_or)\n",
    "        \n",
    "        print(f\"\\nAnzahl Keywords: {len(keywords_with_or.split(' OR ')) if keywords_with_or else 0}\")\n",
    "        \n",
    "        # Speichere Ergebnis in Datei\n",
    "        with open(\"extracted_keywords.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"Ollama Response:\\n\")\n",
    "            f.write(list_response)\n",
    "            f.write(\"\\n\\nExtracted Keywords:\\n\")\n",
    "            f.write(keywords_with_or)\n",
    "        \n",
    "        print(\"\\nErgebnis gespeichert in 'extracted_keywords.txt'\")\n",
    "        \n",
    "        return keywords_with_or\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Fehler: 'dokument.xlsx' nicht gefunden!\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b60310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [400]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json \n",
    "url = \"https://www.searchapi.io/api/v1/search\"\n",
    "API_KEY=\"GAfKZo9ckfQ8xL8Y4H6qTZ1A\"\n",
    "\n",
    "params={\n",
    "\"enigne\":\"google_patents\",\n",
    "\"q\": \"ChatGPT\",\n",
    "\"api_key\":\"GAfKZo9ckfQ8xL8Y4H6qTZ1A\",\n",
    "\n",
    "}\n",
    "\n",
    "response_api = requests.get(f\"https://www.searchapi.io/api/v1/search?api_key={API_KEY}&=google_patents&q=ChatGPT\")\n",
    "\n",
    "print(response)\n",
    "json_file= response.json()\n",
    "\n",
    "with open(\"Test1.json\",\"w\") as file:\n",
    "    json.dump(json_file, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV-Datei erfolgreich eingelesen\n",
      "CSV-Datei erfolgreich eingelesen\n",
      "Sende Anfrage an Ollama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 15:18:24,159 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's tackle this problem. The user has provided a list of patents and wants to know if there are any potential conflicts between them. The final answer should be a list where each entry is either 1 (conflict) or 0 (no conflict). Any other numbers are invalid.\n",
      "\n",
      "First, I need to understand what constitutes a patent conflict. Typically, patent conflicts arise when two patents have overlapping claims, especially if they are in the same technical field and the claims are similar or the same. However, without specific details on the claims of each patent, I might need to rely on other indicators like the same inventors, same assignees, or similar classifications.\n",
      "\n",
      "Looking at the data provided, each patent entry includes details like the patent number, country, status, assignee, inventors, and classification codes. Let me go through each entry and check for possible conflicts.\n",
      "\n",
      "Starting with the first entry (index 0), it's a patent in the US (US9999999999A) with assignee \"MAXELL LTD\" and classifications G01N27/404 and G01N27/416. The next entries are for other countries, but I need to check if there are any overlapping assignees or classifications. For example, entry 3 (index 3) is a Japanese patent (JP2022161670A) with assignee \"MAXELL LTD\" as well. Both have similar classifications (G01N27/404 and G01N27/416). This could indicate a potential conflict because they are from the same company and have similar classifications, possibly related to electrochemical oxygen sensors. However, the exact claims aren't provided, so this is an assumption. The user might consider this a conflict.\n",
      "\n",
      "Another entry (index 4) is a US patent (US10000000000A) with assignee \"MAXELL LTD\" and classifications G01N27/404 and G01N27/416. This is the same assignee and classifications as the first entry, so that's a possible conflict.\n",
      "\n",
      "Looking at entries 5 to 8, they are from different countries (EP, JP, etc.) with different assignees. For example, entry 5 (index 5) has assignee \"THINGS GO ONLINE...\" and classifications related to digital assets. Entry 6 (index 6) has the same assignee and similar classifications. Entries 7 and 8 (indices 7 and 8) also have the same assignee and similar classifications. These might be related to the same technology area (digital assets, blockchain), so there could be conflicts among these entries.\n",
      "\n",
      "Entry 9 (index 9) is another patent from the same assignee as entry 5, with similar classifications. This might indicate a conflict with entries 5, 6, 7, 8, etc.\n",
      "\n",
      "However, the user's instruction says to return 1 for potential conflict and 0 for no conflict. The challenge is that without knowing the exact claims, I have to make educated guesses based on assignees and classifications. For example, if two patents have the same assignee and similar classifications, it's possible they are related, hence a conflict. But if they have different classifications or different assignees, it might not be a conflict.\n",
      "\n",
      "Another point is the note at the end about the last entry (index 10) possibly conflicting with other existing patents. The user mentions that the patentdb.csv might have conflicts, so maybe the answer is that the last entry (index 10) has a conflict.\n",
      "\n",
      "But the user's exact instruction is to return a list where each entry is 1 or 0. The example given in the problem might have conflicting entries. For example, the first entry (index 0) and the third entry (index 3) both have MAXELL LTD as assignee and similar classifications. So, entries 0 and 3 might conflict. Similarly, entries 0 and 4 (same assignee and classifications) would conflict. Entries 5,6,7,8,9 might have conflicts among themselves because they share the same assignee and similar classifications. The last entry (index 10) might conflict with others if it's from the same assignee or similar classifications.\n",
      "\n",
      "But I need to check each pair. However, the user might expect that if two patents have the same assignee and similar classifications, they are considered conflicting. So, for each entry, check if there's another entry with the same assignee and similar classifications.\n",
      "\n",
      "Looking at the entries:\n",
      "\n",
      "- Entry 0: MAXELL LTD, G01N27/404, G01N27/416\n",
      "- Entry 3: MAXELL LTD, G01N27/404, G01N27/416\n",
      "- Entry 4: MAXELL LTD, same classifications\n",
      "- Entry 5: THINGS GO ONLINE..., G06Q... etc.\n",
      "- Entry 6: same assignee as 5, similar classifications\n",
      "- Entry 7: same as 5 and 6\n",
      "- Entry 8: same as 5,6,7\n",
      "- Entry 9: same assignee as 5, similar classifications\n",
      "- Entry 10: MAXELL LTD, G01N27/404, G01N27/416\n",
      "\n",
      "So, entries 0,3,4,10 would conflict with each other because they have the same assignee and classifications. Entries 5,6,7,8,9 would conflict with each other as they have the same assignee and similar classifications.\n",
      "\n",
      "However, the user might consider that if two patents have the same assignee and similar classifications, they are in the same field and could conflict. Therefore, the answer would be a list of 1s for those entries where conflicts are possible.\n",
      "\n",
      "But how many entries are there? The user's input has 11 entries (indices 0 to 10). The final answer should be a list with 11 elements, each being 0 or 1.\n",
      "\n",
      "Wait, the user's instruction says to return a list where 1 indicates a potential conflict and 0 indicates no conflict. The example given in the problem might have conflicting entries. For instance, the last entry (index 10) might conflict with others. However, without more information, the safest approach is to assume that entries with the same assignee and classifications are conflicting.\n",
      "\n",
      "So, entries 0,3,4,10 would conflict with each other. Entries 5,6,7,8,9 would conflict with each other. However, the user might not want to list all conflicts, but perhaps each entry's conflict with any other. But since the user hasn't specified which pairs to check, it's possible that the answer is that the last entry (index 10) conflicts with others, hence 1. Alternatively, the user might expect that entries with the same assignee and classifications have conflicts.\n",
      "\n",
      "But the user's note at the end says that the last entry (index 10) might conflict with other existing patents in patentdb.csv, so the answer could be that the last entry has a conflict (1), and others are 0. However, the user's instruction is to return a list for each entry. \n",
      "\n",
      "Alternatively, maybe the user's example has some entries that conflict. For example, entry 0 and 3 have the same assignee and classifications, so they conflict. Similarly, entry 0 and 4, and 0 and 10. So, each of those entries would have conflicts. But the user might expect that each entry is checked against all others, but that's computationally intensive. \n",
      "\n",
      "Given the ambiguity, perhaps the answer is that entries 0,3,4,10 have conflicts (1), and others 0. However, the user's example might have different expectations. Since the user's note says that the last entry might conflict, the answer could be that the last entry is 1, and others are 0. But without more data, it's hard to say. \n",
      "\n",
      "Alternatively, the user might expect that all entries with the same assignee and classifications have conflicts. So, entries 0,3,4,10 (all MAXELL LTD with G01N classifications) would be 1, and entries 5-9 (THINGS GO ONLINE with similar classifications) would be 1. Others (like entry 1,2, etc.) would be 0. But without knowing the exact claims, it's speculative. \n",
      "\n",
      "In conclusion, based on the information given, the answer would be a list where entries with the same assignee and classifications are marked as 1, others as 0. So, entries 0,3,4,10 are 1, and entries 5-9 are 1, others 0. But the user's example might have different expectations. Given that, the final answer would be a list of 1s and 0s accordingly.\n",
      "</think>\n",
      "\n",
      "[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datenbank=\"patent.csv\"\n",
    "datenbank_df = pd.read_csv(datenbank)\n",
    "datenbank_content = datenbank_df.to_string()\n",
    "print(\"CSV-Datei erfolgreich eingelesen\")\n",
    "        \n",
    "# DataFrame zu Text konvertieren\n",
    "content = df.to_string()\n",
    "\n",
    "datenbank_response_csv=\"patentdb.csv\"\n",
    "datenbank_df = pd.read_csv(datenbank_response_csv)\n",
    "datenbank_content = datenbank_df.to_string()\n",
    "print(\"CSV-Datei erfolgreich eingelesen\")\n",
    "\n",
    "# Prompt für Ollama\n",
    "prompt = f\"Please provide feedback on whether these patents{datenbank_content} might conflict with other existing patents{datenbank_response_csv}. Return a list where 1 indicates a potential conflict and 0 indicates no conflict.Just Print the Final Answer as list other integeres tahn 0,1 are not valid\"\n",
    "        \n",
    "print(\"Sende Anfrage an Ollama...\")\n",
    "response=ollama.generate(model=\"qwen3:8b\", prompt=prompt)\n",
    "list_response=response[\"response\"]\n",
    "print(list_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417c1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 14:48:04,035 - INFO - Erkannte Kodierung: UTF-8-SIG (Konfidenz: 1.00)\n",
      "2025-06-08 14:48:04,035 - WARNING - Versuch 1 fehlgeschlagen: read_csv() got an unexpected keyword argument 'error_bad_lines'. Did you mean 'on_bad_lines'?\n",
      "2025-06-08 14:48:04,036 - WARNING - Versuch 2 fehlgeschlagen: read_csv() got an unexpected keyword argument 'error_bad_lines'. Did you mean 'on_bad_lines'?\n",
      "2025-06-08 14:48:04,036 - WARNING - Versuch 3 fehlgeschlagen: read_csv() got an unexpected keyword argument 'error_bad_lines'. Did you mean 'on_bad_lines'?\n",
      "2025-06-08 14:48:04,037 - WARNING - Versuch 4 fehlgeschlagen: read_csv() got an unexpected keyword argument 'error_bad_lines'. Did you mean 'on_bad_lines'?\n",
      "2025-06-08 14:48:04,037 - WARNING - Versuch 5 fehlgeschlagen: read_csv() got an unexpected keyword argument 'error_bad_lines'. Did you mean 'on_bad_lines'?\n",
      "2025-06-08 14:48:04,037 - WARNING - Versuch 6 fehlgeschlagen: read_csv() got an unexpected keyword argument 'error_bad_lines'. Did you mean 'on_bad_lines'?\n",
      "2025-06-08 14:48:04,038 - WARNING - Versuch 7 fehlgeschlagen: read_csv() got an unexpected keyword argument 'error_bad_lines'. Did you mean 'on_bad_lines'?\n",
      "2025-06-08 14:48:04,038 - INFO - Erkannte Kodierung: UTF-8-SIG (Konfidenz: 1.00)\n",
      "2025-06-08 14:48:04,040 - ERROR - Manuelles Parsen fehlgeschlagen: 39 columns passed, passed data had 75 columns\n",
      "2025-06-08 14:48:04,041 - ERROR - Alle Versuche zum Einlesen von patentdb.csv fehlgeschlagen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versuche CSV-Datei einzulesen...\n",
      "FEHLER: CSV-Datei konnte nicht eingelesen werden\n",
      "Mögliche Ursachen:\n",
      "- Datei 'patentdb.csv' existiert nicht\n",
      "- Kodierungsprobleme\n",
      "- Beschädigte oder unvollständige Daten\n",
      "- Problematische Anführungszeichen oder Trennzeichen\n",
      "\n",
      "==================================================\n",
      "CSV-VALIDIERUNG FEHLGESCHLAGEN\n",
      "==================================================\n",
      "Bitte überprüfen Sie die Datei 'patentdb.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import chardet\n",
    "\n",
    "# Logging konfigurieren\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"Klasse für umfassende Datenvalidierung und Fehlerbehandlung\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_results = {}\n",
    "        \n",
    "    def detect_encoding(self, file_path: str) -> str:\n",
    "        \"\"\"Erkennt die Kodierung einer Datei\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                raw_data = file.read()\n",
    "                result = chardet.detect(raw_data)\n",
    "                encoding = result['encoding']\n",
    "                confidence = result['confidence']\n",
    "                logger.info(f\"Erkannte Kodierung: {encoding} (Konfidenz: {confidence:.2f})\")\n",
    "                return encoding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler bei Kodierungserkennung: {e}\")\n",
    "            return 'utf-8'\n",
    "    \n",
    "    def safe_read_csv(self, file_path: str, **kwargs) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Sicheres Einlesen von CSV-Dateien mit Fehlerbehandlung\"\"\"\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"Datei nicht gefunden: {file_path}\")\n",
    "            return None\n",
    "            \n",
    "        # Standard-Parameter für robustes Einlesen\n",
    "        default_params = {\n",
    "            'encoding': None,\n",
    "            'sep': None,  # Pandas erkennt automatisch\n",
    "            'quotechar': '\"',\n",
    "            'escapechar': '\\\\',\n",
    "            'error_bad_lines': False,\n",
    "            'warn_bad_lines': True,\n",
    "            'on_bad_lines': 'skip',  # Für neuere Pandas-Versionen\n",
    "            'skipinitialspace': True,\n",
    "            'skip_blank_lines': True\n",
    "        }\n",
    "        \n",
    "        # Benutzerdefinierte Parameter überschreiben Standard-Parameter\n",
    "        params = {**default_params, **kwargs}\n",
    "        \n",
    "        # Verschiedene Ansätze ausprobieren\n",
    "        attempts = [\n",
    "            # Versuch 1: Kodierung automatisch erkennen\n",
    "            lambda: self._try_read_csv(file_path, **{**params, 'encoding': self.detect_encoding(file_path)}),\n",
    "            \n",
    "            # Versuch 2: Häufige Kodierungen\n",
    "            lambda: self._try_read_csv(file_path, **{**params, 'encoding': 'utf-8'}),\n",
    "            lambda: self._try_read_csv(file_path, **{**params, 'encoding': 'latin1'}),\n",
    "            lambda: self._try_read_csv(file_path, **{**params, 'encoding': 'cp1252'}),\n",
    "            \n",
    "            # Versuch 3: Verschiedene Trennzeichen\n",
    "            lambda: self._try_read_csv(file_path, **{**params, 'sep': ';', 'encoding': 'utf-8'}),\n",
    "            lambda: self._try_read_csv(file_path, **{**params, 'sep': '\\t', 'encoding': 'utf-8'}),\n",
    "            \n",
    "            # Versuch 4: Quote-Charaktere ignorieren\n",
    "            lambda: self._try_read_csv(file_path, **{**params, 'quoting': 3, 'encoding': 'utf-8'}),  # QUOTE_NONE\n",
    "            \n",
    "            # Versuch 5: Als Text einlesen und manuell parsen\n",
    "            lambda: self._manual_csv_parse(file_path)\n",
    "        ]\n",
    "        \n",
    "        for i, attempt in enumerate(attempts, 1):\n",
    "            try:\n",
    "                df = attempt()\n",
    "                if df is not None and not df.empty:\n",
    "                    logger.info(f\"CSV erfolgreich eingelesen (Versuch {i})\")\n",
    "                    logger.info(f\"Shape: {df.shape}\")\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Versuch {i} fehlgeschlagen: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.error(f\"Alle Versuche zum Einlesen von {file_path} fehlgeschlagen\")\n",
    "        return None\n",
    "    \n",
    "    def _try_read_csv(self, file_path: str, **kwargs) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Einzelner Versuch zum Einlesen einer CSV\"\"\"\n",
    "        return pd.read_csv(file_path, **kwargs)\n",
    "    \n",
    "    def _manual_csv_parse(self, file_path: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Manuelles Parsen für schwierige CSV-Dateien\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=self.detect_encoding(file_path)) as file:\n",
    "                lines = file.readlines()\n",
    "                \n",
    "            # Leere Zeilen entfernen\n",
    "            lines = [line.strip() for line in lines if line.strip()]\n",
    "            \n",
    "            if not lines:\n",
    "                return None\n",
    "                \n",
    "            # Trennzeichen bestimmen\n",
    "            separators = [',', ';', '\\t']\n",
    "            best_sep = ','\n",
    "            max_cols = 0\n",
    "            \n",
    "            for sep in separators:\n",
    "                cols = len(lines[0].split(sep))\n",
    "                if cols > max_cols:\n",
    "                    max_cols = cols\n",
    "                    best_sep = sep\n",
    "            \n",
    "            # Daten parsen\n",
    "            data = []\n",
    "            for line in lines:\n",
    "                # Einfache Behandlung von Anführungszeichen\n",
    "                row = []\n",
    "                cells = line.split(best_sep)\n",
    "                for cell in cells:\n",
    "                    cell = cell.strip().strip('\"').strip(\"'\")\n",
    "                    row.append(cell)\n",
    "                data.append(row)\n",
    "            \n",
    "            # DataFrame erstellen\n",
    "            if data:\n",
    "                df = pd.DataFrame(data[1:], columns=data[0])\n",
    "                logger.info(\"Manuelles Parsen erfolgreich\")\n",
    "                return df\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Manuelles Parsen fehlgeschlagen: {e}\")\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def analyze_csv_structure(self, file_path: str) -> Dict:\n",
    "        \"\"\"Analysiert die Struktur der CSV-Datei vor dem Einlesen\"\"\"\n",
    "        \n",
    "        analysis = {\n",
    "            'file_exists': os.path.exists(file_path),\n",
    "            'file_size': 0,\n",
    "            'encoding': None,\n",
    "            'likely_separator': ',',\n",
    "            'sample_lines': [],\n",
    "            'potential_issues': []\n",
    "        }\n",
    "        \n",
    "        if not analysis['file_exists']:\n",
    "            analysis['potential_issues'].append(\"Datei existiert nicht\")\n",
    "            return analysis\n",
    "            \n",
    "        try:\n",
    "            analysis['file_size'] = os.path.getsize(file_path)\n",
    "            analysis['encoding'] = self.detect_encoding(file_path)\n",
    "            \n",
    "            # Erste Zeilen analysieren\n",
    "            with open(file_path, 'r', encoding=analysis['encoding']) as file:\n",
    "                sample_lines = [file.readline().strip() for _ in range(5)]\n",
    "                analysis['sample_lines'] = [line for line in sample_lines if line]\n",
    "            \n",
    "            # Trennzeichen bestimmen\n",
    "            if analysis['sample_lines']:\n",
    "                separators = [',', ';', '\\t', '|']\n",
    "                sep_counts = {}\n",
    "                for sep in separators:\n",
    "                    sep_counts[sep] = analysis['sample_lines'][0].count(sep)\n",
    "                \n",
    "                analysis['likely_separator'] = max(sep_counts, key=sep_counts.get)\n",
    "                \n",
    "                # Potenzielle Probleme erkennen\n",
    "                first_line = analysis['sample_lines'][0]\n",
    "                if '\"' in first_line and first_line.count('\"') % 2 != 0:\n",
    "                    analysis['potential_issues'].append(\"Unvollständige Anführungszeichen\")\n",
    "                \n",
    "                if any(line.endswith(',') or line.endswith(';') for line in analysis['sample_lines']):\n",
    "                    analysis['potential_issues'].append(\"Zeilen enden mit Trennzeichen\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            analysis['potential_issues'].append(f\"Analysefehler: {e}\")\n",
    "            \n",
    "        return analysis\n",
    "    \n",
    "    def validate_dataframe(self, df: pd.DataFrame, name: str = \"DataFrame\") -> Dict:\n",
    "        \"\"\"Umfassende Validierung eines DataFrames\"\"\"\n",
    "        \n",
    "        validation_results = {\n",
    "            'name': name,\n",
    "            'shape': df.shape,\n",
    "            'columns': list(df.columns),\n",
    "            'dtypes': df.dtypes.to_dict(),\n",
    "            'missing_values': df.isnull().sum().to_dict(),\n",
    "            'duplicate_rows': df.duplicated().sum(),\n",
    "            'memory_usage': df.memory_usage(deep=True).sum(),\n",
    "            'issues': []\n",
    "        }\n",
    "        \n",
    "        # Fehlende Werte prüfen\n",
    "        missing_percentage = (df.isnull().sum() / len(df) * 100)\n",
    "        high_missing = missing_percentage[missing_percentage > 50]\n",
    "        if not high_missing.empty:\n",
    "            validation_results['issues'].append(\n",
    "                f\"Spalten mit >50% fehlenden Werten: {high_missing.to_dict()}\"\n",
    "            )\n",
    "        \n",
    "        # Datentypen prüfen\n",
    "        object_columns = df.select_dtypes(include=['object']).columns\n",
    "        if len(object_columns) > 0:\n",
    "            validation_results['object_columns'] = list(object_columns)\n",
    "            \n",
    "        # Duplikate prüfen\n",
    "        if validation_results['duplicate_rows'] > 0:\n",
    "            validation_results['issues'].append(\n",
    "                f\"{validation_results['duplicate_rows']} doppelte Zeilen gefunden\"\n",
    "            )\n",
    "        \n",
    "        # Spalten mit nur einem Wert\n",
    "        single_value_cols = [col for col in df.columns if df[col].nunique() <= 1]\n",
    "        if single_value_cols:\n",
    "            validation_results['single_value_columns'] = single_value_cols\n",
    "            validation_results['issues'].append(\n",
    "                f\"Spalten mit nur einem Wert: {single_value_cols}\"\n",
    "            )\n",
    "        \n",
    "        self.validation_results[name] = validation_results\n",
    "        logger.info(f\"Validierung für {name} abgeschlossen\")\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Grundlegende Datenbereinigung\"\"\"\n",
    "        \n",
    "        df_cleaned = df.copy()\n",
    "        \n",
    "        # Duplikate entfernen\n",
    "        df_cleaned = df_cleaned.drop_duplicates()\n",
    "        \n",
    "        # Spalten mit nur NaN-Werten entfernen\n",
    "        df_cleaned = df_cleaned.dropna(axis=1, how='all')\n",
    "        \n",
    "        # Zeilen mit nur NaN-Werten entfernen\n",
    "        df_cleaned = df_cleaned.dropna(axis=0, how='all')\n",
    "        \n",
    "        # Spaltennamen bereinigen\n",
    "        df_cleaned.columns = df_cleaned.columns.str.strip()\n",
    "        \n",
    "        logger.info(f\"Datenbereinigung abgeschlossen. Shape: {df.shape} -> {df_cleaned.shape}\")\n",
    "        \n",
    "        return df_cleaned\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generiert einen Validierungsbericht\"\"\"\n",
    "        \n",
    "        report = \"\\n\" + \"=\"*50 + \"\\n\"\n",
    "        report += \"DATENVALIDIERUNGSBERICHT\\n\"\n",
    "        report += \"=\"*50 + \"\\n\"\n",
    "        \n",
    "        for name, results in self.validation_results.items():\n",
    "            report += f\"\\n--- {results['name']} ---\\n\"\n",
    "            report += f\"Shape: {results['shape']}\\n\"\n",
    "            report += f\"Spalten: {len(results['columns'])}\\n\"\n",
    "            report += f\"Doppelte Zeilen: {results['duplicate_rows']}\\n\"\n",
    "            report += f\"Speicherverbrauch: {results['memory_usage'] / 1024:.2f} KB\\n\"\n",
    "            \n",
    "            if results['issues']:\n",
    "                report += \"\\nProbleme gefunden:\\n\"\n",
    "                for issue in results['issues']:\n",
    "                    report += f\"  - {issue}\\n\"\n",
    "            else:\n",
    "                report += \"\\nKeine kritischen Probleme gefunden.\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Hauptfunktion für CSV-Validierung\n",
    "def validate_patent_csv():\n",
    "    \"\"\"Validiert nur die CSV-Datei mit robuster Fehlerbehandlung\"\"\"\n",
    "    \n",
    "    validator = DataValidator()\n",
    "    \n",
    "    # CSV-Datei einlesen\n",
    "    print(\"Versuche CSV-Datei einzulesen...\")\n",
    "    csv_df = validator.safe_read_csv(\"patentdb.csv\")\n",
    "    \n",
    "    if csv_df is not None:\n",
    "        print(\"CSV-Datei erfolgreich eingelesen\")\n",
    "        print(f\"Ursprüngliche Shape: {csv_df.shape}\")\n",
    "        \n",
    "        # Daten bereinigen\n",
    "        csv_df_cleaned = validator.clean_dataframe(csv_df)\n",
    "        print(f\"Shape nach Bereinigung: {csv_df_cleaned.shape}\")\n",
    "        \n",
    "        # Validierung durchführen\n",
    "        validator.validate_dataframe(csv_df_cleaned, \"Patent-Datenbank\")\n",
    "        \n",
    "        # Validierungsbericht ausgeben\n",
    "        print(validator.generate_report())\n",
    "        \n",
    "        return csv_df_cleaned\n",
    "    else:\n",
    "        print(\"FEHLER: CSV-Datei konnte nicht eingelesen werden\")\n",
    "        print(\"Mögliche Ursachen:\")\n",
    "        print(\"- Datei 'patentdb.csv' existiert nicht\")\n",
    "        print(\"- Kodierungsprobleme\")\n",
    "        print(\"- Beschädigte oder unvollständige Daten\")\n",
    "        print(\"- Problematische Anführungszeichen oder Trennzeichen\")\n",
    "        return None\n",
    "\n",
    "# Verwendungsbeispiel\n",
    "if __name__ == \"__main__\":\n",
    "    # Nur CSV validieren\n",
    "    csv_df = validate_patent_csv()\n",
    "    \n",
    "    if csv_df is not None:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"CSV-VALIDIERUNG ERFOLGREICH\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Finale Datenbank Shape: {csv_df.shape}\")\n",
    "        print(f\"Spalten: {list(csv_df.columns)}\")\n",
    "        \n",
    "        # Excel separat einlesen (ohne Validierung)\n",
    "        try:\n",
    "            excel_df = pd.read_excel(\"dokument.xlsx\")\n",
    "            content = excel_df.to_string()\n",
    "            print(\"Excel-Datei erfolgreich eingelesen\")\n",
    "            \n",
    "            # Datenbank-Content für Prompt vorbereiten\n",
    "            datenbank_content = csv_df.to_string()\n",
    "            \n",
    "            prompt = f\"\"\"Please provide feedback on whether these patents:\n",
    "{content}\n",
    "\n",
    "might conflict with other existing patents:\n",
    "{datenbank_content}\n",
    "\n",
    "Return a list where 1 indicates a potential conflict and 0 indicates no conflict.\n",
    "Just Print the Final Answer\"\"\"\n",
    "            \n",
    "            print(\"\\nDaten für Ollama vorbereitet...\")\n",
    "            print(f\"Patent-Dokument Shape: {excel_df.shape}\")\n",
    "            print(f\"Validierte Datenbank Shape: {csv_df.shape}\")\n",
    "            \n",
    "            # Hier Ollama-Aufruf einfügen:\n",
    "            # response = ollama.generate(model=\"qwen3:8b\", prompt=prompt)\n",
    "            # list_response = response[\"response\"]\n",
    "            # print(list_response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Einlesen der Excel-Datei: {e}\")\n",
    "            print(\"Nur CSV-Validierung durchgeführt\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"CSV-VALIDIERUNG FEHLGESCHLAGEN\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"Bitte überprüfen Sie die Datei 'patentdb.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d41d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Winf3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
